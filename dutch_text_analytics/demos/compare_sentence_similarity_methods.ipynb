{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script compares various sentence similarity techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade dutch_text_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dutch_text_analytics.textanalytics as ta\n",
    "from dutch_text_analytics import demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initialising simple extractors\")\n",
    "spacy_extractor = ta.Embedder(method='spacy',suffix='token_based')\n",
    "# doc2vec_extractor = ta.Embedder(method='doc2vec', model_path='models/doc2vec_model')\n",
    "dberttf_extractor = ta.Embedder(method='TF', model_path='distilbert-base-uncased', suffix='dbert-tf')\n",
    "mlbert_extractor = ta.Embedder(method='TF', model_path='distilbert-base-multilingual-cased', suffix='mlbert')\n",
    "glove_extractor = ta.Embedder(method='sentTF', model_path='sentence-transformers/average_word_embeddings_glove.6B.300d', suffix='glove')\n",
    "print(\"initialising transformer extractors\")\n",
    "bert_extractor = ta.Embedder(method='TF', model_path='bert-base-uncased', suffix='bert')\n",
    "robbert_extractor = ta.Embedder(method='TF', model_path='pdelobelle/robbert-v2-dutch-base', suffix='robbert')\n",
    "bertje_extractor = ta.Embedder(method='TF', model_path='GroNLP/bert-base-dutch-cased',suffix='bertje')\n",
    "medrobertanl_extractor = ta.Embedder(method='TF', model_path='CLTL/MedRoBERTa.nl', suffix='medrobertanl')\n",
    "print(\"initialising traditional extractors\")\n",
    "jaccard_extractor = ta.Embedder(method='jaccard')\n",
    "wmd_extractor = ta.Embedder(method='wmd')\n",
    "ld_extractor = ta.Embedder(method='ld')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = demo_data.sentences\n",
    "references = demo_data.references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_public.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_dict = {'sentence':[], 'reference':[], 'method':[], 'comparison_type':[], 'measure':[]}\n",
    "\n",
    "chosen_extractors = [\n",
    "                    spacy_extractor, \n",
    "                    doc2vec_extractor, \n",
    "                    dberttf_extractor,\n",
    "                    mlbert_extractor, \n",
    "                    glove_extractor, \n",
    "                    bert_extractor, \n",
    "                    robbert_extractor, \n",
    "                    bertje_extractor,\n",
    "                    medrobertanl_extractor,\n",
    "                    jaccard_extractor, \n",
    "                    wmd_extractor, \n",
    "                    ld_extractor,\n",
    "                     ]\n",
    "for extractor in chosen_extractors:\n",
    "  print('Model:',extractor.name)\n",
    "  for sentence1 in sentences.keys():\n",
    "      for sentence2 in references:\n",
    "            measure = extractor.get_comparison(sentence1,sentence2)\n",
    "            result_dict['sentence'].append(sentence1)\n",
    "            result_dict['reference'].append(sentence2)\n",
    "            result_dict['method'].append(extractor.name)\n",
    "            result_dict['comparison_type'].append(extractor.comparison_type)\n",
    "            result_dict['measure'].append(measure)\n",
    "        \n",
    "result = pd.DataFrame(result_dict)\n",
    "print(result)    \n",
    "result_file = 'outputs/compare_sentence_similarity_methods_fast.xlsx'\n",
    "result.to_excel(result_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_measures = list(result.method.unique())+['ensemble']\n",
    "all_measure_dict = {i:{'correct_match_count':0, 'partially_correct_match_count':0,'incorrect_match_count':0, 'correct_matches':{},'partially_correct_matches':{},'incorrect_matches':{}} for i in all_measures}\n",
    "N = 2\n",
    "for sen in result.sentence.unique():\n",
    "    for method in all_measures:\n",
    "        for type2 in result.comparison_type.unique():\n",
    "            if method == 'ensemble':\n",
    "                score_df = result[(result.sentence == sen)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            else:\n",
    "                score_df = result[(result.sentence == sen) & (result.method == method)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            if type1 != type2:\n",
    "                continue\n",
    "            if type1 == \"similarity\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=False)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=False)[:N]['measure'].to_list()\n",
    "            if type1 == \"distance\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=True)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=True)[:N]['measure'].to_list()\n",
    "\n",
    "            best_match_stat = [f'Measure:{score:.2f} {match}' for (match,score) in zip(best_matches, best_scores)]\n",
    "            if any(set(best_matches).intersection(set(sentences[sen][:1]))):\n",
    "                all_measure_dict[method][\"correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"correct_matches\"].update({sen:best_match_stat})\n",
    "            elif any(set(best_matches).intersection(set(sentences[sen]))):\n",
    "                all_measure_dict[method][\"partially_correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"partially_correct_matches\"].update({sen:best_match_stat})\n",
    "            else:\n",
    "                # print('**',sen,method,type1,type2,best_matches,'**')\n",
    "                all_measure_dict[method][\"incorrect_match_count\"] += 1\n",
    "                all_measure_dict[method][\"incorrect_matches\"].update({sen:best_match_stat})\n",
    "\n",
    "for k,v in all_measure_dict.items():\n",
    "    print(k)\n",
    "    for k_,v_ in v.items():\n",
    "        print(k_)\n",
    "        if type(v_) == dict:\n",
    "            for k__,v__ in v_.items():\n",
    "                print('\\t',k__,':',)\n",
    "                if type(v__) == list:\n",
    "                    for i in v__:\n",
    "                        print('\\t\\t:',i)\n",
    "        else:\n",
    "            print('\\t',k_,':',v_)\n",
    "    print(\"************************\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\")\n",
    "summary_dict = {'method_name':[], 'correct_match_count':[], 'partially_correct_match_count':[], 'incorrect_match_count':[]}\n",
    "for k,v in all_measure_dict.items():\n",
    "    summary_dict['method_name'].append(k)\n",
    "    for k_,v_ in v.items():\n",
    "        if k_ in ['correct_match_count','partially_correct_match_count','incorrect_match_count',]:\n",
    "            summary_dict[k_].append(v_)\n",
    "summary_df = pd.DataFrame(summary_dict)\n",
    "print(summary_df)\n",
    "summary_file = 'outputs/compare_sentence_similarity_methods_fast_summary.xlsx'\n",
    "summary_df.to_excel(summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textanalytics import Classifier\n",
    "mdeberta_classifier = Classifier(method='TF', classes = list(references), model_path=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", suffix=\"mDeBERTa\", multi_label=False)\n",
    "\n",
    "analysis_dict = {'sentence':[],'gt':[], 'prediction':[]}\n",
    "for sentence,gt_labels in sentences.items():\n",
    "    gt_label = gt_labels[0]\n",
    "    result = mdeberta_classifier.classify(sentence)\n",
    "    predicted_label = result['labels'][np.argmax(result['scores'])]\n",
    "    analysis_dict['sentence'].append(sentence)\n",
    "    analysis_dict['gt'].append(gt_label) \n",
    "    analysis_dict['prediction'].append(predicted_label)   \n",
    "analysis_df =  pd.DataFrame(analysis_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
