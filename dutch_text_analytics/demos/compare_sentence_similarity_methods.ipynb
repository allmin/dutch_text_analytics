{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script compares various sentence similarity techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade dutch_text_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dutch_text_analytics.textanalytics as ta\n",
    "from dutch_text_analytics import demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising simple extractors\n",
      "initialising transformer extractors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at CLTL/MedRoBERTa.nl and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising traditional extractors\n"
     ]
    }
   ],
   "source": [
    "print(\"initialising simple extractors\")\n",
    "spacy_extractor = ta.Embedder(method='spacy',suffix='token_based')\n",
    "# doc2vec_extractor = ta.Embedder(method='doc2vec', model_path='models/doc2vec_model')\n",
    "dberttf_extractor = ta.Embedder(method='TF', model_path='distilbert-base-uncased', suffix='dbert-tf')\n",
    "mlbert_extractor = ta.Embedder(method='TF', model_path='distilbert-base-multilingual-cased', suffix='mlbert')\n",
    "glove_extractor = ta.Embedder(method='sentTF', model_path='sentence-transformers/average_word_embeddings_glove.6B.300d', suffix='glove')\n",
    "print(\"initialising transformer extractors\")\n",
    "bert_extractor = ta.Embedder(method='TF', model_path='bert-base-uncased', suffix='bert')\n",
    "robbert_extractor = ta.Embedder(method='TF', model_path='pdelobelle/robbert-v2-dutch-base', suffix='robbert')\n",
    "bertje_extractor = ta.Embedder(method='TF', model_path='GroNLP/bert-base-dutch-cased',suffix='bertje')\n",
    "medrobertanl_extractor = ta.Embedder(method='TF', model_path='CLTL/MedRoBERTa.nl', suffix='medrobertanl')\n",
    "print(\"initialising traditional extractors\")\n",
    "jaccard_extractor = ta.Embedder(method='jaccard')\n",
    "wmd_extractor = ta.Embedder(method='wmd')\n",
    "ld_extractor = ta.Embedder(method='ld')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = demo_data.sentences\n",
    "references = demo_data.references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc2vec_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison_type\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeasure\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[1;32m      3\u001b[0m chosen_extractors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m                     spacy_extractor, \n\u001b[0;32m----> 5\u001b[0m                     \u001b[43mdoc2vec_extractor\u001b[49m, \n\u001b[1;32m      6\u001b[0m                     dberttf_extractor,\n\u001b[1;32m      7\u001b[0m                     mlbert_extractor, \n\u001b[1;32m      8\u001b[0m                     glove_extractor, \n\u001b[1;32m      9\u001b[0m                     bert_extractor, \n\u001b[1;32m     10\u001b[0m                     robbert_extractor, \n\u001b[1;32m     11\u001b[0m                     bertje_extractor,\n\u001b[1;32m     12\u001b[0m                     medrobertanl_extractor,\n\u001b[1;32m     13\u001b[0m                     jaccard_extractor, \n\u001b[1;32m     14\u001b[0m                     wmd_extractor, \n\u001b[1;32m     15\u001b[0m                     ld_extractor,\n\u001b[1;32m     16\u001b[0m                      ]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m extractor \u001b[38;5;129;01min\u001b[39;00m chosen_extractors:\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel:\u001b[39m\u001b[38;5;124m'\u001b[39m,extractor\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc2vec_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "result_dict = {'sentence':[], 'reference':[], 'method':[], 'comparison_type':[], 'measure':[]}\n",
    "\n",
    "chosen_extractors = [\n",
    "                    spacy_extractor, \n",
    "                    dberttf_extractor,\n",
    "                    mlbert_extractor, \n",
    "                    glove_extractor, \n",
    "                    bert_extractor, \n",
    "                    robbert_extractor, \n",
    "                    bertje_extractor,\n",
    "                    medrobertanl_extractor,\n",
    "                    jaccard_extractor, \n",
    "                    wmd_extractor, \n",
    "                    ld_extractor,\n",
    "                     ]\n",
    "for extractor in chosen_extractors:\n",
    "  print('Model:',extractor.name)\n",
    "  for sentence1 in sentences.keys():\n",
    "      for sentence2 in references:\n",
    "            measure = extractor.get_comparison(sentence1,sentence2)\n",
    "            result_dict['sentence'].append(sentence1)\n",
    "            result_dict['reference'].append(sentence2)\n",
    "            result_dict['method'].append(extractor.name)\n",
    "            result_dict['comparison_type'].append(extractor.comparison_type)\n",
    "            result_dict['measure'].append(measure)\n",
    "        \n",
    "result = pd.DataFrame(result_dict)\n",
    "print(result)    \n",
    "if os.path.exists('output'):\n",
    "  os.makedirs('output')\n",
    "result_file = 'outputs/compare_sentence_similarity_methods_fast.xlsx'\n",
    "result.to_excel(result_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_measures = list(result.method.unique())+['ensemble']\n",
    "all_measure_dict = {i:{'correct_match_count':0, 'partially_correct_match_count':0,'incorrect_match_count':0, 'correct_matches':{},'partially_correct_matches':{},'incorrect_matches':{}} for i in all_measures}\n",
    "N = 2\n",
    "for sen in result.sentence.unique():\n",
    "    for method in all_measures:\n",
    "        for type2 in result.comparison_type.unique():\n",
    "            if method == 'ensemble':\n",
    "                score_df = result[(result.sentence == sen)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            else:\n",
    "                score_df = result[(result.sentence == sen) & (result.method == method)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            if type1 != type2:\n",
    "                continue\n",
    "            if type1 == \"similarity\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=False)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=False)[:N]['measure'].to_list()\n",
    "            if type1 == \"distance\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=True)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=True)[:N]['measure'].to_list()\n",
    "\n",
    "            best_match_stat = [f'Measure:{score:.2f} {match}' for (match,score) in zip(best_matches, best_scores)]\n",
    "            if any(set(best_matches).intersection(set(sentences[sen][:1]))):\n",
    "                all_measure_dict[method][\"correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"correct_matches\"].update({sen:best_match_stat})\n",
    "            elif any(set(best_matches).intersection(set(sentences[sen]))):\n",
    "                all_measure_dict[method][\"partially_correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"partially_correct_matches\"].update({sen:best_match_stat})\n",
    "            else:\n",
    "                # print('**',sen,method,type1,type2,best_matches,'**')\n",
    "                all_measure_dict[method][\"incorrect_match_count\"] += 1\n",
    "                all_measure_dict[method][\"incorrect_matches\"].update({sen:best_match_stat})\n",
    "\n",
    "for k,v in all_measure_dict.items():\n",
    "    print(k)\n",
    "    for k_,v_ in v.items():\n",
    "        print(k_)\n",
    "        if type(v_) == dict:\n",
    "            for k__,v__ in v_.items():\n",
    "                print('\\t',k__,':',)\n",
    "                if type(v__) == list:\n",
    "                    for i in v__:\n",
    "                        print('\\t\\t:',i)\n",
    "        else:\n",
    "            print('\\t',k_,':',v_)\n",
    "    print(\"************************\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\")\n",
    "summary_dict = {'method_name':[], 'correct_match_count':[], 'partially_correct_match_count':[], 'incorrect_match_count':[]}\n",
    "for k,v in all_measure_dict.items():\n",
    "    summary_dict['method_name'].append(k)\n",
    "    for k_,v_ in v.items():\n",
    "        if k_ in ['correct_match_count','partially_correct_match_count','incorrect_match_count',]:\n",
    "            summary_dict[k_].append(v_)\n",
    "summary_df = pd.DataFrame(summary_dict)\n",
    "print(summary_df)\n",
    "summary_file = 'outputs/compare_sentence_similarity_methods_fast_summary.xlsx'\n",
    "summary_df.to_excel(summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textanalytics import Classifier\n",
    "mdeberta_classifier = Classifier(method='TF', classes = list(references), model_path=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", suffix=\"mDeBERTa\", multi_label=False)\n",
    "\n",
    "analysis_dict = {'sentence':[],'gt':[], 'prediction':[]}\n",
    "for sentence,gt_labels in sentences.items():\n",
    "    gt_label = gt_labels[0]\n",
    "    result = mdeberta_classifier.classify(sentence)\n",
    "    predicted_label = result['labels'][np.argmax(result['scores'])]\n",
    "    analysis_dict['sentence'].append(sentence)\n",
    "    analysis_dict['gt'].append(gt_label) \n",
    "    analysis_dict['prediction'].append(predicted_label)   \n",
    "analysis_df =  pd.DataFrame(analysis_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
