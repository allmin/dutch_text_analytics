{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script compares various sentence similarity techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "%pip install -U pip\n",
    "%pip install -U spacy\n",
    "%pip install -U gensim\n",
    "%pip install pandas numpy scipy scikit-learn spacy nltk transformers torch gensim openpyxl python-Levenshtein sent2vec sentence-transformers\n",
    "!python -m spacy download nl_core_news_lg\n",
    "import nltk\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import torch\n",
    "import Levenshtein\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "if os.path.exists('text_private.py'):\n",
    "    from text_private import sentences, references\n",
    "elif os.path.exists('text_public.py'):\n",
    "    from text_public import sentences, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    def __init__(self, method, model_path=None, suffix=''):\n",
    "        self.method = method\n",
    "        self.suffix = suffix\n",
    "        if suffix!=\"\":\n",
    "            self.name = f\"{method}_{suffix}\"\n",
    "        else:\n",
    "            self.name = self.method\n",
    "        self.model_path = model_path\n",
    "        self.embedding_dict = {}\n",
    "        self.comparison_type = 'similarity'\n",
    "        self.nlp = spacy.load('nl_core_news_lg')\n",
    "        self.initialise_model()\n",
    "\n",
    "    def initialise_model(self):\n",
    "        if self.method=='spacy':\n",
    "            self.model = self.nlp\n",
    "        elif self.method=='doc2vec':\n",
    "            self.model = Doc2Vec.load(self.model_path)\n",
    "        elif self.method=='sent2vec':\n",
    "            self.model = Vectorizer(pretrained_weights=self.model_path)\n",
    "        elif\tself.method=='sentTF':\n",
    "            self.model = SentenceTransformer(self.model_path)\n",
    "        elif self.method == 'TF':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = AutoModel.from_pretrained(self.model_path)\n",
    "        elif self.method == 'wmd':\n",
    "            self.comparison_type = 'distance'\n",
    "        elif self.method == 'ld':\n",
    "            self.comparison_type = 'distance' \n",
    "            \n",
    "    def generate_embedding(self,sentence):\n",
    "        if self.method == 'spacy':\n",
    "            tokens = self.model(sentence)\n",
    "            vector = np.median([word.vector for word in tokens], axis=0)\n",
    "        elif self.method == 'doc2vec':\n",
    "            tokens = self.get_tokens(sentence)\n",
    "            tagged_data = [TaggedDocument(words=tokens, tags=[\"sentence\"])]\n",
    "            vector = self.model.infer_vector(tagged_data[0].words)\n",
    "        elif self.method == 'sent2vec':\n",
    "            if type(sentence) == list:\n",
    "                self.model.__init__(self.model_path)\n",
    "                sentences = sentence\n",
    "                self.model.run(sentences)\n",
    "                vector = self.model.vectors\n",
    "            else:\n",
    "                self.model.__init__(self.model_path)\n",
    "                sentences = [sentence]\n",
    "                self.model.run(sentences)\n",
    "                vector = self.model.vectors[0]\n",
    "        elif self.method == 'sentTF':\n",
    "            if type(sentence) == list:\n",
    "                self.model.__init__(self.model_path)\n",
    "                sentences = sentence\n",
    "                vector = self.model.encode(sentences)\n",
    "            else:\n",
    "                self.model.__init__(self.model_path)\n",
    "                sentences = [sentence]\n",
    "                vector = self.model.encode(sentences)[0]\n",
    "        elif self.method == 'TF':\n",
    "            inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids,attention_mask=attention_mask)\n",
    "                vector = outputs.last_hidden_state.mean(dim=1).numpy()[0]\n",
    "        return vector\n",
    "    \n",
    "    def get_embedding(self, sentence):\n",
    "        if type(sentence) == str:\n",
    "            if not self.is_exist(sentence):\n",
    "                embedding = self.generate_embedding(sentence)\n",
    "                self.embedding_dict[sentence] = embedding\n",
    "            else:\n",
    "                embedding = self.embedding_dict[sentence]\n",
    "            return embedding\n",
    "        elif type(sentence) == list:\n",
    "            embeddings = self.generate_embedding(sentence)\n",
    "            for sen,embedding in zip(sentence,embeddings):\n",
    "                self.embedding_dict[sen] = embedding\n",
    "    \n",
    "    def is_exist(self,sentence):\n",
    "        if sentence in self.embedding_dict.keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_comparison(self,sentence1,sentence2):\n",
    "        if self.method in ['spacy','doc2vec','sent2vec','sentTF', 'TF']:\n",
    "            self.vec1 = self.get_embedding(sentence1)\n",
    "            self.vec2 = self.get_embedding(sentence2)\n",
    "            measure = cosine_similarity([self.vec1], [self.vec2])[0][0]\n",
    "        elif self.method == 'wmd':\n",
    "            tokens1 = self.nlp(sentence1)\n",
    "            tokens2 = self.nlp(sentence2)\n",
    "            measure = tokens1.similarity(tokens2)\n",
    "        elif self.method=='jaccard':\n",
    "            tokens1 = self.get_tokens(sentence1)\n",
    "            tokens2 = self.get_tokens(sentence2)\n",
    "            set1 = set(tokens1)\n",
    "            set2 = set(tokens2)\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1) + len(set2) - intersection\n",
    "            measure = intersection / union\n",
    "        elif self.method == 'ld':\n",
    "            measure = Levenshtein.distance(sentence1, sentence2)\n",
    "        return measure\n",
    "    \n",
    "    def get_tokens(self,sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        tokens = [token.text for token in doc]\n",
    "        return tokens\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initialising simple extractors\")\n",
    "spacy_extractor = Embedder(method='spacy',suffix='token_based')\n",
    "doc2vec_extractor = Embedder(method='doc2vec', model_path='models/doc2vec_model')\n",
    "# dbert_extractor = Embedder(method='sent2vec', model_path='distilbert-base-uncased', suffix='dbert')\n",
    "dberttf_extractor = Embedder(method='TF', model_path='distilbert-base-uncased', suffix='dbert-tf')\n",
    "# mlbert_extractor = Embedder(method='sent2vec', model_path='distilbert-base-multilingual-cased', suffix='mlbert')\n",
    "mlbert_extractor = Embedder(method='TF', model_path='distilbert-base-multilingual-cased', suffix='mlbert')\n",
    "glove_extractor = Embedder(method='sentTF', model_path='sentence-transformers/average_word_embeddings_glove.6B.300d', suffix='glove')\n",
    "print(\"initialising transformer extractors\")\n",
    "bert_extractor = Embedder(method='TF', model_path='bert-base-uncased', suffix='bert')\n",
    "robbert_extractor = Embedder(method='TF', model_path='pdelobelle/robbert-v2-dutch-base', suffix='robbert')\n",
    "bertje_extractor = Embedder(method='TF', model_path='GroNLP/bert-base-dutch-cased',suffix='bertje')\n",
    "print(\"initialising traditional extractors\")\n",
    "jaccard_extractor = Embedder(method='jaccard')\n",
    "wmd_extractor = Embedder(method='wmd')\n",
    "ld_extractor = Embedder(method='ld')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_dict = {'sentence':[], 'reference':[], 'method':[], 'comparison_type':[], 'measure':[]}\n",
    "\n",
    "chosen_extractors = [\n",
    "                    spacy_extractor, \n",
    "                    doc2vec_extractor, \n",
    "                    dberttf_extractor,\n",
    "                    mlbert_extractor, \n",
    "                    glove_extractor, \n",
    "                    bert_extractor, \n",
    "                    robbert_extractor, \n",
    "                    bertje_extractor,\n",
    "                    jaccard_extractor, \n",
    "                    wmd_extractor, \n",
    "                    ld_extractor,\n",
    "                     ]\n",
    "for extractor in chosen_extractors:\n",
    "  print('Model:',extractor.name)\n",
    "  for sentence1 in sentences.keys():\n",
    "      for sentence2 in references:\n",
    "            measure = extractor.get_comparison(sentence1,sentence2)\n",
    "            result_dict['sentence'].append(sentence1)\n",
    "            result_dict['reference'].append(sentence2)\n",
    "            result_dict['method'].append(extractor.name)\n",
    "            result_dict['comparison_type'].append(extractor.comparison_type)\n",
    "            result_dict['measure'].append(measure)\n",
    "        \n",
    "result = pd.DataFrame(result_dict)\n",
    "print(result)    \n",
    "result_file = 'compare_sentence_similarity_methods_fast.xlsx'\n",
    "result.to_excel(result_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_measures = list(result.method.unique())+['ensemble']\n",
    "all_measure_dict = {i:{'correct_match_count':0, 'partially_correct_match_count':0,'incorrect_match_count':0, 'correct_matches':{},'partially_correct_matches':{},'incorrect_matches':{}} for i in all_measures}\n",
    "N = 3\n",
    "for sen in result.sentence.unique():\n",
    "    for method in all_measures:\n",
    "        for type2 in result.comparison_type.unique():\n",
    "            if method == 'ensemble':\n",
    "                score_df = result[(result.sentence == sen)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            else:\n",
    "                score_df = result[(result.sentence == sen) & (result.method == method)]\n",
    "                type1 = score_df.comparison_type.unique()[0]\n",
    "            if type1 != type2:\n",
    "                pass\n",
    "            if type1 == \"similarity\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=False)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=False)[:N]['measure'].to_list()\n",
    "            if type1 == \"distance\":\n",
    "                best_matches = score_df.sort_values(['measure'], ascending=True)[:N]['reference'].to_list()\n",
    "                best_scores = score_df.sort_values(['measure'], ascending=True)[:N]['measure'].to_list()\n",
    "\n",
    "            best_match_stat = [f'Measure:{score:.2f} {match}' for (match,score) in zip(best_matches, best_scores)]\n",
    "            if any(set(best_matches).intersection(set(sentences[sen][:1]))):\n",
    "                all_measure_dict[method][\"correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"correct_matches\"].update({sen:best_match_stat})\n",
    "            elif any(set(best_matches).intersection(set(sentences[sen]))):\n",
    "                all_measure_dict[method][\"partially_correct_match_count\"] += 1\n",
    "                all_measure_dict[method][\"partially_correct_matches\"].update({sen:best_match_stat})\n",
    "            else:\n",
    "                all_measure_dict[method][\"incorrect_match_count\"] += 1\n",
    "                all_measure_dict[method][\"incorrect_matches\"].update({sen:best_match_stat})\n",
    "\n",
    "for k,v in all_measure_dict.items():\n",
    "    print(k)\n",
    "    for k_,v_ in v.items():\n",
    "        print(k_)\n",
    "        if type(v_) == dict:\n",
    "            for k__,v__ in v_.items():\n",
    "                print('\\t',k__,':',)\n",
    "                if type(v__) == list:\n",
    "                    for i in v__:\n",
    "                        print('\\t\\t:',i)\n",
    "        else:\n",
    "            print('\\t',k_,':',v_)\n",
    "    print(\"************************\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
